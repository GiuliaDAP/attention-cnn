{
  "version": 3,
  "file": "bundle.css",
  "sources": [
    "../../src/App.svelte"
  ],
  "sourcesContent": [
    "<script>\n  import _ from \"lodash-es\";\n\n  import Grid from \"./Grid.svelte\";\n\n  const ModelTypes = {\n    RELATIVE: \"relative\",\n    POSITION_ONLY: \"position_only\",\n    VISION_TRANSFORMER: \"vision_transformer\"\n  };\n\n  $: numberColumns = modelType == ModelTypes.VISION_TRANSFORMER ? 14 : 16;\n  let modelType = ModelTypes.VISION_TRANSFORMER;\n  let withSoftmax = true;\n\n  const relativeDisplayValues = [\n    [\"attention (qr+qk)\", \"attention\"],\n    [\"positional only scores (qr)\", \"qTr\"],\n    [\"content only scores (qk)\", \"qTk\"]\n  ];\n\n  function pad(num, size) {\n    var s = \"0000000000000000\" + num;\n    return s.substr(s.length - size);\n  }\n\n  const imagePath = {\n    [ModelTypes.RELATIVE]: index =>\n      \"./cifar10-test-images/image_\" + index + \".jpg\",\n    [ModelTypes.POSITION_ONLY]: index =>\n      \"./cifar10-test-images/image_\" + index + \".jpg\",\n    [ModelTypes.VISION_TRANSFORMER]: index =>\n      index == \"batch\"\n        ? \"./cifar10-test-images/image_batch.jpg\"\n        : \"./imagenet/image_\" + pad(index, 3) + \".png\"\n  };\n\n  const imageIndices = {\n    [ModelTypes.RELATIVE]: [\"batch\", 2, 3, 19, 56],\n    [ModelTypes.POSITION_ONLY]: [\"position\"],\n    [ModelTypes.VISION_TRANSFORMER]: [\"batch\", 3, 40, 42, 55, 88]\n  };\n\n  let selectedContentAttentionType;\n\n  $: selectedColumn = Math.round(numberColumns / 3);\n  $: selectedRow = Math.round(numberColumns / 4);\n\n  const onChangeRowColumn = (row, col) => {\n    selectedRow = row;\n    selectedColumn = col;\n  };\n\n  let selectedImages = {};\n  selectedImages[ModelTypes.RELATIVE] = \"batch\";\n  selectedImages[ModelTypes.POSITION_ONLY] = \"position\";\n  selectedImages[ModelTypes.VISION_TRANSFORMER] = \"batch\";\n\n  $: selectedImage = selectedImages[modelType];\n\n  const squareSize = 6;\n\n  const onSelectImage = imageIndex => {\n    selectedImages[modelType] = imageIndex;\n  };\n\n  const pairsToDict = listOfPairs => {\n    return _.zipObject(..._.zip(...listOfPairs));\n  };\n\n  $: filename = filenameAt(selectedRow, selectedColumn);\n\n  $: filenameAt = (col, row) => {\n    let position;\n    if (modelType == ModelTypes.VISION_TRANSFORMER)\n      position = col == null ? \"cls\" : row + \"_\" + col;\n    else position = col == null ? \"cls\" : col + \"_\" + row;\n    const image_filename =\n      \"image_\" + selectedImage + \"/img_\" + position + \".png\";\n    if (modelType == ModelTypes.RELATIVE) {\n      let displayValues = selectedContentAttentionType;\n      if (displayValues == \"attention\") {\n        displayValues = displayValues + (withSoftmax ? \"_probs\" : \"_scores\");\n      }\n      return \"png/learned_qk_qr/\" + displayValues + \"/\" + image_filename;\n    } else if (modelType == ModelTypes.POSITION_ONLY)\n      return (\n        \"png/learned/attention\" +\n        (withSoftmax ? \"_probs\" : \"_scores\") +\n        \"/\" +\n        image_filename\n      );\n    else if (modelType == ModelTypes.VISION_TRANSFORMER)\n      return \"png/vit/\" + image_filename;\n  };\n\n  const urlParams = new URLSearchParams(window.location.search);\n  const isVitOnly = urlParams.has('vitonly');\n  const isRelOnly = urlParams.has('relonly');\n\n  if (isRelOnly) {\n    modelType = ModelTypes.RELATIVE;\n  }\n</script>\n\n<style>\n  .sampleImage {\n    margin-right: 12px;\n    margin-bottom: 8px;\n    position: relative;\n    float: left;\n    /* border: 1px solid black; */\n  }\n\n  .imageSelected {\n    box-shadow: 3px 3px 3px #69a9d2bb;\n    outline: none;\n  }\n\n  .selected {\n    box-shadow: 0 0 5px #2980b9;\n    /* border: 3px solid #2980b9; */\n    outline: none;\n  }\n\n  /* move the grid on the image it lies over */\n  .grid {\n    top: 0;\n    left: 0;\n    position: absolute;\n  }\n\n  select {\n    display: inline;\n  }\n\n  div.clearFloat {\n    clear: both;\n  }\n\n  .attentionMaps {\n    width: 100%;\n  }\n\n  .preload img {\n    opacity: 0;\n    position: absolute;\n    width: 0;\n    height: 0;\n  }\n\n  .modelTypeSelectBox {\n    border: 1px solid grey;\n    /* outline: 2px solid white; */\n    padding: 0.5em 0.8em;\n    margin-right: 1em;\n    margin-bottom: 0.5em;\n    border-radius: 0.5em;\n    float: left;\n    user-select: none;\n  }\n\n  .modelTypeSelectBox:last-child {\n    margin-right: 0;\n  }\n\n  .modelTypeSelectBox.selected {\n    box-shadow: 0 0 5px #2980b9;\n  }\n\n  .modelTypeSelectBox h4 {\n    margin-top: 0;\n    margin-bottom: 0.3em;\n    font-variant: small-caps;\n    /* font-weight: normal; */\n  }\n\n  .imagesContainer {\n    margin-left: 3%;\n  }\n</style>\n\n<main>\n  <div style={(isVitOnly || isRelOnly) ? \"display: none\" : \"\"}>\n  <h1>Visualization of Self-Attention Maps in Vision</h1>\n\n  <p>\n    This interactive webpage illustrates the findings of our paper\n    <a href=\"https://openreview.net/pdf?id=HJlnC1rKPB\">\n      On the Relationship between Self-Attention and Convolutional Layers\n    </a>\n    published at ICLR 2020. We prove that a Self-Attention layer can express any\n    convolution (under basic conditions met in practice) by attending on (groups\n    of) pixels at fixed shift of the query pixel. This expressivity result is\n    somehow matched in practice for some heads that ignore the content and\n    compute a peak attention probability at a fixed shift (up to a symmetry).\n  </p>\n\n  <p>\n    This page displays interactive attention maps computed by a 6-layer\n    self-attention model trained to classify CIFAR-10 images. You can consult\n    our\n    <a href=\"http://jbcordonnier.com/posts/attention-cnn/\">blog post</a>\n    for a gentle introduction to our paper. The code is available on\n    <a href=\"https://github.com/epfml/attention-cnn\">Github</a>\n    , the experimental setting is detailed in the paper.\n  </p>\n\n  <p>\n    <b>Edit 4/12/2020:</b>\n    We added the visualization of\n    <a href=\"https://arxiv.org/abs/2010.11929\">Vision Transformer.</a>\n    We used the implementation from\n    <a href=\"https://github.com/rwightman/pytorch-image-models\">timm</a>\n    and the weights from the\n    <a href=\"https://github.com/google-research/vision_transformer\">\n      original repository.\n    </a>\n    ViT-Base/16 is a larger model trained on ImageNet rather than CIFAR-10\n    without any image specific architecture choice in the positional encoding.\n  </p>\n\n  <h4>Select attention type:</h4>\n  <div class=\"modelTypeSelect\">\n    <div\n      class=\"modelTypeSelectBox\"\n      style=\"width:26%\"\n      on:click={() => {\n        modelType = ModelTypes.RELATIVE;\n      }}\n      class:selected={modelType == ModelTypes.RELATIVE}>\n      <h4>Relative Self-Attention</h4>\n      Use 2D relative positional encoding and image content to compute the\n      attention.\n    </div>\n    <div\n      class=\"modelTypeSelectBox\"\n      style=\"width:30%\"\n      on:click={() => {\n        modelType = ModelTypes.POSITION_ONLY;\n      }}\n      class:selected={modelType == ModelTypes.POSITION_ONLY}>\n      <h4>Position-only Self-Attention</h4>\n      Discard the pixel values and compute the attention scores only on relative\n      positions.\n    </div>\n    <div\n      class=\"modelTypeSelectBox\"\n      style=\"width:29%\"\n      on:click={() => {\n        modelType = ModelTypes.VISION_TRANSFORMER;\n      }}\n      class:selected={modelType == ModelTypes.VISION_TRANSFORMER}>\n      <h4>Vision Transformer</h4>\n      Use absolute 1D positional encoding and CLS token for classification.\n      ViT-Base/16.\n    </div>\n  </div>\n\n  <div class=\"clearFloat\" />\n</div>\n<div style={(isVitOnly || isRelOnly) ? \"display: none\" : \"\"}>\n{#if modelType == ModelTypes.RELATIVE}\n    <br />\n    <b>Display values:</b>\n    <select bind:value={selectedContentAttentionType}>\n      {#each relativeDisplayValues as [display, value]}\n        <option type=\"radio\" {value}>{display}</option>\n      {/each}\n    </select>\n    {#if selectedContentAttentionType == 'attention'}\n      <b>with softmax</b>\n      <input type=\"checkbox\" bind:checked={withSoftmax} />\n    {:else}\n      <b>without softmax</b>\n    {/if}\n  {:else if modelType == ModelTypes.POSITION_ONLY}\n    <br />\n    <b>Display positional attention</b>\n    <input type=\"checkbox\" bind:checked={withSoftmax} />\n    <b>with softmax</b>\n    {/if}\n  </div>\n\n  <h4 style={(isVitOnly || isRelOnly) ? \"display: none\" : \"\"}>Select image and query pixel:</h4>\n\n  <div class=\"imagesContainer\">\n    {#each imageIndices[modelType] as index}\n      <div\n        class=\"sampleImage\"\n        style={'height: ' + numberColumns * squareSize + 'px;' + 'padding-left:' + (modelType == ModelTypes.VISION_TRANSFORMER ? squareSize : 0) + 'px'}>\n        <img\n          src={imagePath[modelType](index)}\n          alt=\"\"\n          class:imageSelected={index === selectedImage}\n          width={numberColumns * squareSize}\n          on:click={() => onSelectImage(index)} />\n        {#if selectedImage == index}\n          <div class=\"grid\">\n            <Grid\n              displayCLSToken={modelType == ModelTypes.VISION_TRANSFORMER}\n              columns={numberColumns}\n              rows={numberColumns}\n              {selectedColumn}\n              {selectedRow}\n              {onChangeRowColumn}\n              size={squareSize}\n              class=\"grid\" />\n          </div>\n        {/if}\n      </div>\n    {/each}\n  </div>\n\n  <div class=\"clearFloat\" />\n\n  <h4 style={(isVitOnly || isRelOnly) ? \"display: none\" : \"\"}>Visualize attention per layer and head</h4>\n\n  <img src={filename} alt=\"\" class=\"attentionMaps\" />\n\n  <div class=\"preload\">\n    {#each _.range(numberColumns) as col}\n      {#each _.range(numberColumns) as row}\n        <img src={filenameAt(col, row)} alt=\"preload\" />\n      {/each}\n    {/each}\n  </div>\n\n</main>\n"
  ],
  "names": [],
  "mappings": "AA0GE,YAAY,eAAC,CAAC,AACZ,YAAY,CAAE,IAAI,CAClB,aAAa,CAAE,GAAG,CAClB,QAAQ,CAAE,QAAQ,CAClB,KAAK,CAAE,IAAI,AAEb,CAAC,AAED,cAAc,eAAC,CAAC,AACd,UAAU,CAAE,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,SAAS,CACjC,OAAO,CAAE,IAAI,AACf,CAAC,AAED,SAAS,eAAC,CAAC,AACT,UAAU,CAAE,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO,CAE3B,OAAO,CAAE,IAAI,AACf,CAAC,AAGD,KAAK,eAAC,CAAC,AACL,GAAG,CAAE,CAAC,CACN,IAAI,CAAE,CAAC,CACP,QAAQ,CAAE,QAAQ,AACpB,CAAC,AAED,MAAM,eAAC,CAAC,AACN,OAAO,CAAE,MAAM,AACjB,CAAC,AAED,GAAG,WAAW,eAAC,CAAC,AACd,KAAK,CAAE,IAAI,AACb,CAAC,AAED,cAAc,eAAC,CAAC,AACd,KAAK,CAAE,IAAI,AACb,CAAC,AAED,uBAAQ,CAAC,GAAG,eAAC,CAAC,AACZ,OAAO,CAAE,CAAC,CACV,QAAQ,CAAE,QAAQ,CAClB,KAAK,CAAE,CAAC,CACR,MAAM,CAAE,CAAC,AACX,CAAC,AAED,mBAAmB,eAAC,CAAC,AACnB,MAAM,CAAE,GAAG,CAAC,KAAK,CAAC,IAAI,CAEtB,OAAO,CAAE,KAAK,CAAC,KAAK,CACpB,YAAY,CAAE,GAAG,CACjB,aAAa,CAAE,KAAK,CACpB,aAAa,CAAE,KAAK,CACpB,KAAK,CAAE,IAAI,CACX,WAAW,CAAE,IAAI,AACnB,CAAC,AAED,kCAAmB,WAAW,AAAC,CAAC,AAC9B,YAAY,CAAE,CAAC,AACjB,CAAC,AAED,mBAAmB,SAAS,eAAC,CAAC,AAC5B,UAAU,CAAE,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO,AAC7B,CAAC,AAED,kCAAmB,CAAC,EAAE,eAAC,CAAC,AACtB,UAAU,CAAE,CAAC,CACb,aAAa,CAAE,KAAK,CACpB,YAAY,CAAE,UAAU,AAE1B,CAAC,AAED,gBAAgB,eAAC,CAAC,AAChB,WAAW,CAAE,EAAE,AACjB,CAAC"
}